1. Modelul bag-of-words
➔ este o metodă de reprezentare a datelor de tip text, bazată pe frecvența
de apariție a cuvintelor în cadrul documentelor
➔ algoritmul este alcătuit din 2 pași:
1. definirea unui vocabular prin atribuirea unui id unic fiecărui
cuvânt regăsit în setul de date (setul de antrenare)
2. reprezentarea fiecărui document ca un vector de dimensiune
egală cu lungimea vocabularului, definit astfel:
𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠[𝑤𝑜𝑟𝑑_𝑖𝑑𝑥] = 𝑛𝑢𝑚ă𝑟𝑢𝑙 𝑑𝑒 𝑎𝑝𝑎𝑟𝑖ț𝑖𝑖 𝑎𝑙 𝑐𝑢𝑣â𝑛𝑡𝑢𝑙𝑢𝑖 𝑐𝑢 𝑖𝑑 − 𝑢𝑙 𝑤𝑜𝑟𝑑_𝑖𝑑𝑥

Functii folosite:
➔ def normalize_data(train_data, test_data, type = None):   primeste datele de intrare si iesire si intoarce aceste date normalizate in functie de tip: STANDARD, min_max, l1,l2 l2
➔def build_vocabulary(self,data): primeste ca parametru o lista de mesaje si construieste vocabularul be baza acesteia
➔def get_features(self,data): primeste ca parametru o lista de mesaje si returneaza o matrice definita astfel: 𝒇𝒆𝒂𝒕𝒖𝒓𝒆𝒔(𝒔𝒂𝒎𝒑𝒍𝒆_𝒊𝒅𝒙,𝒘𝒐𝒓𝒅_𝒊𝒅𝒙) = 𝒏𝒖𝒎𝒂𝒓𝒖𝒍 𝒅𝒆 𝒂𝒑𝒂𝒓𝒊𝒕𝒊𝒊 𝒂𝒍
 𝒄𝒖𝒗𝒂𝒏𝒕𝒖𝒍𝒖𝒊 𝒄𝒖 𝒊𝒅− 𝒖𝒍 𝒘𝒐𝒓𝒅_𝒊𝒅𝒙 𝒊𝒏 𝒅𝒐𝒄𝒖𝒎𝒆𝒏𝒕𝒖𝒍 𝒔𝒂𝒎𝒑𝒍𝒆_𝒊𝒅𝒙
➔def coefficients(classifier, feature_names, top_features=10): Afișeaza cele mai negative (spam) 10 cuvinte și cele mai pozitive (non-spam) 10
cuvinte.

➔Build the vocabulary:
    BOW = BagOfWords()
    BOW.build_vocabulary(training_sentences)

➔Transforming text into numerical features:
    train_feat = BOW.get_features(training_sentences)
    test_feat = BOW.get_features(test_sentences)
    
➔Normalizing numerical features:
    norm_data = normalize_data(train_feat,test_feat,'l2')
    
➔SVM model training:
    obj = svm.SVC(1,kernel = 'linear')
    obj.fit(norm_data[0],training_labels)
    
➔Predictions:
    predictions = obj.predict(norm_data[1])
 
➔Accuracy:
    accur = accuracy_score(test_labels,predictions)

➔F1-score:
    f1_scor = f1_score(test_labels,predictions,average =None)
    
➔Printing the most important features (negative and positive):
    coefficients(obj, norm_data[1])
